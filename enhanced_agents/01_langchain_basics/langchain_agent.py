"""
LangChainé›†æˆçš„ç®€å•å“åº”Agent
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨LangChainåˆ›å»ºåŸºç¡€çš„å¯¹è¯Agent
"""

import logging
from typing import List, Dict, Any, Optional
from datetime import datetime

from langchain.schema import HumanMessage, SystemMessage, AIMessage
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferMemory

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from common.llm_factory import LLMFactory, create_llm_with_cost_tracking
from common.model_config import ModelConfig, global_cost_tracker

logger = logging.getLogger(__name__)


class LangChainSimpleAgent:
    """åŸºäºLangChainçš„ç®€å•å“åº”Agent"""
    
    def __init__(
        self, 
        provider: str = "openai", 
        model: str = "gpt-3.5-turbo",
        system_prompt: str = None,
        enable_memory: bool = True,
        enable_cost_tracking: bool = True,
        **llm_kwargs
    ):
        """
        åˆå§‹åŒ–Agent
        
        Args:
            provider: LLMæä¾›å•†
            model: æ¨¡å‹åç§°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            enable_memory: æ˜¯å¦å¯ç”¨è®°å¿†åŠŸèƒ½
            enable_cost_tracking: æ˜¯å¦å¯ç”¨æˆæœ¬è¿½è¸ª
        """
        self.provider = provider
        self.model = model
        self.enable_memory = enable_memory
        self.enable_cost_tracking = enable_cost_tracking
        
        # é»˜è®¤ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = system_prompt or self._get_default_system_prompt()
        
        # åˆ›å»ºLLMå®ä¾‹
        if enable_cost_tracking:
            self.llm = create_llm_with_cost_tracking(provider, model, **llm_kwargs)
        else:
            self.llm = LLMFactory.create_llm(provider, model, **llm_kwargs)
        
        # è®¾ç½®è®°å¿†
        if enable_memory:
            self.memory = ConversationBufferMemory(
                return_messages=True,
                memory_key="chat_history"
            )
        else:
            self.memory = None
        
        # åˆ›å»ºæç¤ºæ¨¡æ¿
        self.prompt_template = self._create_prompt_template()
        
        # å¯¹è¯å†å²
        self.conversation_history = []
        
        logger.info(f"LangChain Agent initialized with {provider}:{model}")
    
    def _get_default_system_prompt(self) -> str:
        """è·å–é»˜è®¤ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä¸ªå‹å–„ã€æœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚ä½ èƒ½å¤Ÿï¼š

1. å›ç­”å„ç§é—®é¢˜
2. è¿›è¡Œè‡ªç„¶å¯¹è¯
3. æä¾›å»ºè®®å’Œè§£å†³æ–¹æ¡ˆ
4. ååŠ©å®Œæˆå„ç§ä»»åŠ¡

è¯·å§‹ç»ˆï¼š
- ä¿æŒç¤¼è²Œå’Œä¸“ä¸š
- æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„ä¿¡æ¯
- åœ¨ä¸ç¡®å®šæ—¶è¯šå®è¡¨è¾¾
- é€‚å½“ä½¿ç”¨ä¸­æ–‡å›å¤ä¸­æ–‡é—®é¢˜ï¼Œè‹±æ–‡å›å¤è‹±æ–‡é—®é¢˜

å½“å‰æ—¶é—´ï¼š{current_time}
"""
    
    def _create_prompt_template(self) -> ChatPromptTemplate:
        """åˆ›å»ºæç¤ºæ¨¡æ¿"""
        if self.enable_memory:
            return ChatPromptTemplate.from_messages([
                ("system", self.system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}")
            ])
        else:
            return ChatPromptTemplate.from_messages([
                ("system", self.system_prompt),
                ("human", "{input}")
            ])
    
    def respond(self, user_input: str, **kwargs) -> str:
        """
        å“åº”ç”¨æˆ·è¾“å…¥
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            AIå“åº”
        """
        try:
            # å‡†å¤‡æ¶ˆæ¯
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            if self.enable_memory and self.memory:
                # ä½¿ç”¨è®°å¿†
                messages = self.prompt_template.format_messages(
                    input=user_input,
                    chat_history=self.memory.chat_memory.messages,
                    current_time=current_time
                )
            else:
                # ä¸ä½¿ç”¨è®°å¿†
                messages = self.prompt_template.format_messages(
                    input=user_input,
                    current_time=current_time
                )
            
            # è°ƒç”¨LLM
            response = self.llm.invoke(messages)
            response_text = response.content
            
            # ä¿å­˜åˆ°è®°å¿†
            if self.enable_memory and self.memory:
                self.memory.chat_memory.add_user_message(user_input)
                self.memory.chat_memory.add_ai_message(response_text)
            
            # ä¿å­˜åˆ°å¯¹è¯å†å²
            self.conversation_history.append({
                "timestamp": datetime.now(),
                "user_input": user_input,
                "ai_response": response_text
            })
            
            logger.info(f"Generated response for input: {user_input[:50]}...")
            return response_text
            
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}"
    
    def batch_respond(self, inputs: List[str]) -> List[str]:
        """æ‰¹é‡å“åº”"""
        responses = []
        for user_input in inputs:
            response = self.respond(user_input)
            responses.append(response)
        return responses
    
    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """è·å–å¯¹è¯å†å²"""
        return self.conversation_history
    
    def clear_memory(self):
        """æ¸…é™¤è®°å¿†"""
        if self.memory:
            self.memory.clear()
        self.conversation_history = []
        logger.info("Memory and conversation history cleared")
    
    def get_memory_summary(self) -> str:
        """è·å–è®°å¿†æ‘˜è¦"""
        if not self.enable_memory or not self.memory:
            return "è®°å¿†åŠŸèƒ½æœªå¯ç”¨"
        
        messages = self.memory.chat_memory.messages
        if not messages:
            return "æš‚æ— å¯¹è¯è®°å½•"
        
        return f"å¯¹è¯è½®æ•°ï¼š{len(messages)//2}ï¼Œæœ€è¿‘æ¶ˆæ¯ï¼š{messages[-1].content[:100]}..."
    
    def get_cost_summary(self) -> Dict[str, Any]:
        """è·å–æˆæœ¬æ‘˜è¦"""
        if not self.enable_cost_tracking:
            return {"message": "æˆæœ¬è¿½è¸ªæœªå¯ç”¨"}
        
        return global_cost_tracker.get_cost_summary()
    
    def get_agent_info(self) -> Dict[str, Any]:
        """è·å–Agentä¿¡æ¯"""
        return {
            "provider": self.provider,
            "model": self.model,
            "system_prompt": self.system_prompt[:100] + "...",
            "memory_enabled": self.enable_memory,
            "cost_tracking_enabled": self.enable_cost_tracking,
            "conversation_turns": len(self.conversation_history),
            "memory_summary": self.get_memory_summary(),
        }
    
    def update_system_prompt(self, new_prompt: str):
        """æ›´æ–°ç³»ç»Ÿæç¤ºè¯"""
        self.system_prompt = new_prompt
        self.prompt_template = self._create_prompt_template()
        logger.info("System prompt updated")
    
    def switch_model(self, provider: str, model: str, **kwargs):
        """åˆ‡æ¢æ¨¡å‹"""
        try:
            if self.enable_cost_tracking:
                self.llm = create_llm_with_cost_tracking(provider, model, **kwargs)
            else:
                self.llm = LLMFactory.create_llm(provider, model, **kwargs)
            
            self.provider = provider
            self.model = model
            
            logger.info(f"Switched to model: {provider}:{model}")
            
        except Exception as e:
            logger.error(f"Failed to switch model: {e}")
            raise


class MultiModelAgent:
    """å¤šæ¨¡å‹Agentï¼Œå¯ä»¥åŠ¨æ€é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹"""
    
    def __init__(self):
        self.agents = {}
        self.default_agent = None
        self._initialize_available_agents()
    
    def _initialize_available_agents(self):
        """åˆå§‹åŒ–å¯ç”¨çš„Agent"""
        available_providers = LLMFactory.get_available_providers()
        
        for provider, available in available_providers.items():
            if available:
                models = ModelConfig.list_available_models(provider)[provider]
                for model in models:
                    try:
                        agent_key = f"{provider}:{model}"
                        self.agents[agent_key] = LangChainSimpleAgent(
                            provider=provider,
                            model=model
                        )
                        if not self.default_agent:
                            self.default_agent = self.agents[agent_key]
                        logger.info(f"Initialized agent: {agent_key}")
                    except Exception as e:
                        logger.warning(f"Failed to initialize {agent_key}: {e}")
    
    def respond(self, user_input: str, preferred_model: str = None) -> Dict[str, Any]:
        """ä½¿ç”¨æŒ‡å®šæˆ–æœ€é€‚åˆçš„æ¨¡å‹å“åº”"""
        if preferred_model and preferred_model in self.agents:
            agent = self.agents[preferred_model]
        else:
            # ç®€å•çš„æ¨¡å‹é€‰æ‹©é€»è¾‘
            if "ä»£ç " in user_input or "code" in user_input.lower():
                agent = self._get_best_agent_for_task("coding")
            elif "åˆ†æ" in user_input or "analysis" in user_input.lower():
                agent = self._get_best_agent_for_task("analysis")
            else:
                agent = self.default_agent
        
        if not agent:
            return {
                "response": "æŠ±æ­‰ï¼Œæ²¡æœ‰å¯ç”¨çš„æ¨¡å‹",
                "model_used": "none",
                "success": False
            }
        
        try:
            response = agent.respond(user_input)
            return {
                "response": response,
                "model_used": f"{agent.provider}:{agent.model}",
                "success": True,
                "cost_summary": agent.get_cost_summary()
            }
        except Exception as e:
            return {
                "response": f"é”™è¯¯ï¼š{str(e)}",
                "model_used": f"{agent.provider}:{agent.model}",
                "success": False
            }
    
    def _get_best_agent_for_task(self, task_type: str) -> Optional['LangChainSimpleAgent']:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è·å–æœ€ä½³Agent"""
        provider, model = ModelConfig.get_recommended_model(task_type)
        agent_key = f"{provider}:{model}"
        return self.agents.get(agent_key, self.default_agent)
    
    def list_available_models(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        return list(self.agents.keys())
    
    def get_cost_summary(self) -> Dict[str, Any]:
        """è·å–æ€»æˆæœ¬æ‘˜è¦"""
        return global_cost_tracker.get_cost_summary()


# ä¾¿æ·å‡½æ•°
def create_simple_agent(provider: str = "openai", model: str = "gpt-3.5-turbo") -> LangChainSimpleAgent:
    """å¿«é€Ÿåˆ›å»ºç®€å•Agent"""
    return LangChainSimpleAgent(provider=provider, model=model)


def quick_chat(message: str, provider: str = "openai", model: str = "gpt-3.5-turbo") -> str:
    """å¿«é€Ÿå¯¹è¯"""
    agent = create_simple_agent(provider, model)
    return agent.respond(message)


if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    try:
        print("ğŸ¤– LangChain Agent Test")
        print("=" * 50)
        
        # åˆ›å»ºAgent
        agent = LangChainSimpleAgent()
        print(f"âœ… Agent created: {agent.provider}:{agent.model}")
        
        # è·å–Agentä¿¡æ¯
        info = agent.get_agent_info()
        print(f"ğŸ“Š Agent Info: {info}")
        
        # æµ‹è¯•å¯¹è¯
        test_messages = [
            "ä½ å¥½ï¼",
            "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
        ]
        
        for msg in test_messages:
            print(f"\nğŸ‘¤ ç”¨æˆ·: {msg}")
            response = agent.respond(msg)
            print(f"ğŸ¤– Agent: {response}")
        
        # æ˜¾ç¤ºæˆæœ¬ä¿¡æ¯
        cost_summary = agent.get_cost_summary()
        print(f"\nğŸ’° æˆæœ¬æ‘˜è¦: {cost_summary}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥æ˜¯å¦å·²å®‰è£…å¿…è¦çš„ä¾èµ–å’Œé…ç½®APIå¯†é’¥")