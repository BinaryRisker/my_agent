"""
æ•°æ®åˆ†æAgentä¸»ç¨‹åº
æ•´åˆæ•°æ®å¤„ç†ã€å¯è§†åŒ–å’Œæœºå™¨å­¦ä¹ åŠŸèƒ½ï¼Œæä¾›å…¨é¢çš„æ•°æ®ç§‘å­¦è§£å†³æ–¹æ¡ˆ
"""

import sys
import argparse
import json
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
import pandas as pd
import numpy as np

# Third-party imports
from loguru import logger
import gradio as gr

# Project imports
sys.path.append(str(Path(__file__).parent.parent.parent))
from common.config import get_config

from data_processor import DataProcessor
from ml_engine import MLEngine


class DataAnalysisAgent:
    """æ•°æ®åˆ†æAgentä¸»ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®åˆ†æAgent"""
        self.config = get_config()
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.data_processor = DataProcessor()
        self.ml_engine = MLEngine()
        
        # ä¼šè¯çŠ¶æ€
        self.session_state = {
            'loaded_datasets': [],
            'trained_models': [],
            'analysis_history': [],
            'visualization_history': []
        }
        
        logger.info("æ•°æ®åˆ†æAgentåˆå§‹åŒ–å®Œæˆ")
    
    def load_data_from_source(self,
                            source: Union[str, Dict],
                            data_type: str = "auto",
                            name: Optional[str] = None,
                            **kwargs) -> Dict[str, Any]:
        """
        ä»æ•°æ®æºåŠ è½½æ•°æ®
        
        Args:
            source: æ•°æ®æºï¼ˆæ–‡ä»¶è·¯å¾„æˆ–æ•°æ®å­—å…¸ï¼‰
            data_type: æ•°æ®ç±»å‹
            name: æ•°æ®é›†åç§°
            **kwargs: åŠ è½½å‚æ•°
            
        Returns:
            åŠ è½½ç»“æœ
        """
        try:
            dataset_name = self.data_processor.load_data(source, data_type, name, **kwargs)
            
            # è·å–æ•°æ®ä¿¡æ¯
            data_info = self.data_processor.get_data_info(dataset_name)
            
            # æ›´æ–°ä¼šè¯çŠ¶æ€
            if dataset_name not in self.session_state['loaded_datasets']:
                self.session_state['loaded_datasets'].append(dataset_name)
            
            result = {
                'dataset_name': dataset_name,
                'success': True,
                'data_info': data_info,
                'message': f'æ•°æ®é›† {dataset_name} åŠ è½½æˆåŠŸ'
            }
            
            logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {dataset_name}")
            return result
            
        except Exception as e:
            logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'message': f'æ•°æ®åŠ è½½å¤±è´¥: {e}'
            }
    
    def perform_data_analysis(self,
                            dataset_name: str,
                            analysis_type: str,
                            **kwargs) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ•°æ®åˆ†æ
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            analysis_type: åˆ†æç±»å‹
            **kwargs: åˆ†æå‚æ•°
            
        Returns:
            åˆ†æç»“æœ
        """
        try:
            analysis_result = self.data_processor.analyze_data(
                dataset_name, analysis_type, **kwargs
            )
            
            # è®°å½•åˆ†æå†å²
            self.session_state['analysis_history'].append({
                'dataset_name': dataset_name,
                'analysis_type': analysis_type,
                'parameters': kwargs,
                'timestamp': pd.Timestamp.now().isoformat()
            })
            
            result = {
                'dataset_name': dataset_name,
                'analysis_type': analysis_type,
                'success': True,
                'result': analysis_result,
                'message': f'{analysis_type} åˆ†æå®Œæˆ'
            }
            
            logger.info(f"æ•°æ®åˆ†æå®Œæˆ: {dataset_name} - {analysis_type}")
            return result
            
        except Exception as e:
            logger.error(f"æ•°æ®åˆ†æå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'message': f'æ•°æ®åˆ†æå¤±è´¥: {e}'
            }
    
    def create_data_visualization(self,
                                dataset_name: str,
                                viz_type: str,
                                **kwargs) -> Dict[str, Any]:
        """
        åˆ›å»ºæ•°æ®å¯è§†åŒ–
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            viz_type: å¯è§†åŒ–ç±»å‹
            **kwargs: å¯è§†åŒ–å‚æ•°
            
        Returns:
            å¯è§†åŒ–ç»“æœ
        """
        try:
            viz_html = self.data_processor.create_visualization(
                dataset_name, viz_type, **kwargs
            )
            
            # è®°å½•å¯è§†åŒ–å†å²
            self.session_state['visualization_history'].append({
                'dataset_name': dataset_name,
                'viz_type': viz_type,
                'parameters': kwargs,
                'timestamp': pd.Timestamp.now().isoformat()
            })
            
            result = {
                'dataset_name': dataset_name,
                'viz_type': viz_type,
                'success': True,
                'html': viz_html,
                'message': f'{viz_type} å¯è§†åŒ–åˆ›å»ºå®Œæˆ'
            }
            
            logger.info(f"å¯è§†åŒ–åˆ›å»ºå®Œæˆ: {dataset_name} - {viz_type}")
            return result
            
        except Exception as e:
            logger.error(f"å¯è§†åŒ–åˆ›å»ºå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'message': f'å¯è§†åŒ–åˆ›å»ºå¤±è´¥: {e}'
            }
    
    def clean_and_transform_data(self,
                               dataset_name: str,
                               operations: List[Dict[str, Any]],
                               transformations: List[Dict[str, Any]] = None,
                               create_new: bool = False) -> Dict[str, Any]:
        """
        æ•°æ®æ¸…æ´—å’Œè½¬æ¢
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            operations: æ¸…æ´—æ“ä½œåˆ—è¡¨
            transformations: è½¬æ¢æ“ä½œåˆ—è¡¨
            create_new: æ˜¯å¦åˆ›å»ºæ–°æ•°æ®é›†
            
        Returns:
            å¤„ç†ç»“æœ
        """
        try:
            result_name = dataset_name
            
            # æ•°æ®æ¸…æ´—
            if operations:
                clean_result = self.data_processor.clean_data(
                    dataset_name, operations, create_new
                )
                if clean_result:
                    result_name = clean_result
                    if result_name not in self.session_state['loaded_datasets']:
                        self.session_state['loaded_datasets'].append(result_name)
            
            # æ•°æ®è½¬æ¢
            if transformations:
                transform_result = self.data_processor.transform_data(
                    result_name, transformations, create_new and not operations
                )
                if transform_result:
                    result_name = transform_result
                    if result_name not in self.session_state['loaded_datasets']:
                        self.session_state['loaded_datasets'].append(result_name)
            
            # è·å–å¤„ç†åçš„æ•°æ®ä¿¡æ¯
            data_info = self.data_processor.get_data_info(result_name)
            
            result = {
                'original_dataset': dataset_name,
                'result_dataset': result_name,
                'success': True,
                'data_info': data_info,
                'message': f'æ•°æ®å¤„ç†å®Œæˆ: {result_name}'
            }
            
            logger.info(f"æ•°æ®å¤„ç†å®Œæˆ: {dataset_name} -> {result_name}")
            return result
            
        except Exception as e:
            logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'message': f'æ•°æ®å¤„ç†å¤±è´¥: {e}'
            }
    
    def train_ml_model(self,
                      model_name: str,
                      algorithm: str,
                      task_type: str,
                      dataset_name: str,
                      target_column: Optional[str] = None,
                      test_size: float = 0.2,
                      **kwargs) -> Dict[str, Any]:
        """
        è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            algorithm: ç®—æ³•åç§°
            task_type: ä»»åŠ¡ç±»å‹
            dataset_name: æ•°æ®é›†åç§°
            target_column: ç›®æ ‡åˆ—å
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            **kwargs: ç®—æ³•å‚æ•°
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        try:
            # è·å–æ•°æ®é›†
            if dataset_name not in self.data_processor.datasets:
                raise ValueError(f"æ•°æ®é›†ä¸å­˜åœ¨: {dataset_name}")
            
            df = self.data_processor.datasets[dataset_name]
            
            if task_type in ["classification", "regression"]:
                if not target_column:
                    raise ValueError(f"{task_type} ä»»åŠ¡éœ€è¦æŒ‡å®šç›®æ ‡åˆ—")
                
                if target_column not in df.columns:
                    raise ValueError(f"ç›®æ ‡åˆ—ä¸å­˜åœ¨: {target_column}")
                
                # åˆ†å‰²æ•°æ®
                X = df.drop(columns=[target_column])
                y = df[target_column]
                
                from sklearn.model_selection import train_test_split
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y, test_size=test_size, random_state=42
                )
                
                # è®­ç»ƒæ¨¡å‹
                train_result = self.ml_engine.train_model(
                    model_name, algorithm, task_type, X_train, y_train, **kwargs
                )
                
                # è¯„ä¼°æ¨¡å‹
                eval_result = self.ml_engine.evaluate_model(model_name, X_test, y_test)
                
                result = {
                    'model_name': model_name,
                    'algorithm': algorithm,
                    'task_type': task_type,
                    'dataset_name': dataset_name,
                    'success': True,
                    'train_result': train_result,
                    'eval_result': eval_result,
                    'message': f'æ¨¡å‹ {model_name} è®­ç»ƒå®Œæˆ'
                }
                
            else:  # clustering
                X = df
                
                train_result = self.ml_engine.train_model(
                    model_name, algorithm, task_type, X, **kwargs
                )
                
                result = {
                    'model_name': model_name,
                    'algorithm': algorithm,
                    'task_type': task_type,
                    'dataset_name': dataset_name,
                    'success': True,
                    'train_result': train_result,
                    'message': f'èšç±»æ¨¡å‹ {model_name} è®­ç»ƒå®Œæˆ'
                }
            
            # æ›´æ–°ä¼šè¯çŠ¶æ€
            if model_name not in self.session_state['trained_models']:
                self.session_state['trained_models'].append(model_name)
            
            logger.info(f"æ¨¡å‹è®­ç»ƒå®Œæˆ: {model_name}")
            return result
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'message': f'æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}'
            }
    
    def predict_with_model(self,
                          model_name: str,
                          dataset_name: str,
                          return_probabilities: bool = False) -> Dict[str, Any]:
        """
        ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            dataset_name: æ•°æ®é›†åç§°
            return_probabilities: æ˜¯å¦è¿”å›æ¦‚ç‡
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        try:
            # è·å–æ•°æ®é›†
            if dataset_name not in self.data_processor.datasets:
                raise ValueError(f"æ•°æ®é›†ä¸å­˜åœ¨: {dataset_name}")
            
            df = self.data_processor.datasets[dataset_name]
            
            # æ‰§è¡Œé¢„æµ‹
            prediction_result = self.ml_engine.predict(
                model_name, df, return_probabilities
            )
            
            result = {
                'model_name': model_name,
                'dataset_name': dataset_name,
                'success': True,
                'prediction_result': prediction_result,
                'message': f'é¢„æµ‹å®Œæˆ: {model_name}'
            }
            
            logger.info(f"é¢„æµ‹å®Œæˆ: {model_name} on {dataset_name}")
            return result
            
        except Exception as e:
            logger.error(f"é¢„æµ‹å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'message': f'é¢„æµ‹å¤±è´¥: {e}'
            }
    
    def get_data_summary(self) -> Dict[str, Any]:
        """è·å–æ•°æ®æ‘˜è¦"""
        datasets_info = []
        
        for dataset_name in self.session_state['loaded_datasets']:
            try:
                info = self.data_processor.get_data_info(dataset_name)
                datasets_info.append(info)
            except Exception as e:
                logger.warning(f"è·å–æ•°æ®é›†ä¿¡æ¯å¤±è´¥: {dataset_name}, {e}")
        
        return {
            'loaded_datasets': len(self.session_state['loaded_datasets']),
            'datasets_info': datasets_info,
            'trained_models': len(self.session_state['trained_models']),
            'models_info': self.ml_engine.list_models(),
            'analysis_count': len(self.session_state['analysis_history']),
            'visualization_count': len(self.session_state['visualization_history'])
        }
    
    def export_analysis_results(self,
                              output_path: str,
                              include_data: bool = True,
                              include_models: bool = True) -> Dict[str, Any]:
        """
        å¯¼å‡ºåˆ†æç»“æœ
        
        Args:
            output_path: è¾“å‡ºè·¯å¾„
            include_data: æ˜¯å¦åŒ…å«æ•°æ®
            include_models: æ˜¯å¦åŒ…å«æ¨¡å‹
            
        Returns:
            å¯¼å‡ºç»“æœ
        """
        try:
            output_path = Path(output_path)
            output_path.mkdir(parents=True, exist_ok=True)
            
            exported_files = []
            
            # å¯¼å‡ºæ•°æ®é›†
            if include_data:
                for dataset_name in self.session_state['loaded_datasets']:
                    try:
                        data_path = output_path / f"{dataset_name}.csv"
                        success = self.data_processor.export_data(
                            dataset_name, data_path, "csv"
                        )
                        if success:
                            exported_files.append(str(data_path))
                    except Exception as e:
                        logger.warning(f"å¯¼å‡ºæ•°æ®é›†å¤±è´¥: {dataset_name}, {e}")
            
            # å¯¼å‡ºæ¨¡å‹
            if include_models:
                for model_name in self.session_state['trained_models']:
                    try:
                        model_path = output_path / f"{model_name}.pkl"
                        success = self.ml_engine.save_model(model_name, model_path)
                        if success:
                            exported_files.append(str(model_path))
                    except Exception as e:
                        logger.warning(f"å¯¼å‡ºæ¨¡å‹å¤±è´¥: {model_name}, {e}")
            
            # å¯¼å‡ºä¼šè¯æ‘˜è¦
            summary_path = output_path / "analysis_summary.json"
            summary = self.get_data_summary()
            summary['analysis_history'] = self.session_state['analysis_history']
            summary['visualization_history'] = self.session_state['visualization_history']
            
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False, default=str)
            
            exported_files.append(str(summary_path))
            
            result = {
                'success': True,
                'output_path': str(output_path),
                'exported_files': exported_files,
                'message': f'åˆ†æç»“æœå·²å¯¼å‡ºåˆ°: {output_path}'
            }
            
            logger.info(f"åˆ†æç»“æœå¯¼å‡ºå®Œæˆ: {output_path}")
            return result
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºåˆ†æç»“æœå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'message': f'å¯¼å‡ºå¤±è´¥: {e}'
            }


def create_gradio_interface(agent: DataAnalysisAgent):
    """åˆ›å»ºGradio Webç•Œé¢"""
    
    def load_data_interface(file_path: str, data_type: str, dataset_name: str):
        """æ•°æ®åŠ è½½ç•Œé¢"""
        try:
            if not file_path:
                return "è¯·é€‰æ‹©æ•°æ®æ–‡ä»¶", "", ""
            
            result = agent.load_data_from_source(
                file_path,
                data_type if data_type != "auto" else "auto",
                dataset_name if dataset_name.strip() else None
            )
            
            if result['success']:
                data_info = json.dumps(result['data_info'], indent=2, ensure_ascii=False)
                sample = agent.data_processor.get_dataset_sample(result['dataset_name'])
                sample_str = json.dumps(sample, indent=2, ensure_ascii=False)
                
                return result['message'], data_info, sample_str
            else:
                return result['message'], "", ""
                
        except Exception as e:
            return f"åŠ è½½å¤±è´¥: {e}", "", ""
    
    def analyze_data_interface(dataset_name: str, analysis_type: str):
        """æ•°æ®åˆ†æç•Œé¢"""
        try:
            if not dataset_name:
                return "è¯·é€‰æ‹©æ•°æ®é›†"
            
            result = agent.perform_data_analysis(dataset_name, analysis_type)
            
            if result['success']:
                return json.dumps(result['result'], indent=2, ensure_ascii=False)
            else:
                return result['message']
                
        except Exception as e:
            return f"åˆ†æå¤±è´¥: {e}"
    
    def visualize_data_interface(dataset_name: str, viz_type: str, 
                               column: str, x_column: str, y_column: str):
        """æ•°æ®å¯è§†åŒ–ç•Œé¢"""
        try:
            if not dataset_name:
                return "è¯·é€‰æ‹©æ•°æ®é›†", ""
            
            # æ„å»ºå¯è§†åŒ–å‚æ•°
            kwargs = {}
            if column:
                kwargs['column'] = column
            if x_column:
                kwargs['x_column'] = x_column
            if y_column:
                kwargs['y_column'] = y_column
            
            result = agent.create_data_visualization(dataset_name, viz_type, **kwargs)
            
            if result['success']:
                return result['message'], result['html']
            else:
                return result['message'], ""
                
        except Exception as e:
            return f"å¯è§†åŒ–å¤±è´¥: {e}", ""
    
    def clean_data_interface(dataset_name: str, operations_json: str):
        """æ•°æ®æ¸…æ´—ç•Œé¢"""
        try:
            if not dataset_name:
                return "è¯·é€‰æ‹©æ•°æ®é›†", ""
            
            if not operations_json.strip():
                return "è¯·è¾“å…¥æ¸…æ´—æ“ä½œ", ""
            
            operations = json.loads(operations_json)
            if not isinstance(operations, list):
                operations = [operations]
            
            result = agent.clean_and_transform_data(dataset_name, operations, create_new=True)
            
            if result['success']:
                info = json.dumps(result['data_info'], indent=2, ensure_ascii=False)
                return result['message'], info
            else:
                return result['message'], ""
                
        except Exception as e:
            return f"æ¸…æ´—å¤±è´¥: {e}", ""
    
    def train_model_interface(model_name: str, algorithm: str, task_type: str,
                            dataset_name: str, target_column: str, params_json: str):
        """æ¨¡å‹è®­ç»ƒç•Œé¢"""
        try:
            if not all([model_name, algorithm, task_type, dataset_name]):
                return "è¯·å¡«å†™æ‰€æœ‰å¿…éœ€å­—æ®µ"
            
            if task_type in ["classification", "regression"] and not target_column:
                return "ç›‘ç£å­¦ä¹ ä»»åŠ¡éœ€è¦æŒ‡å®šç›®æ ‡åˆ—"
            
            # è§£æå‚æ•°
            kwargs = {}
            if params_json.strip():
                kwargs = json.loads(params_json)
            
            result = agent.train_ml_model(
                model_name, algorithm, task_type, dataset_name,
                target_column if target_column else None,
                **kwargs
            )
            
            if result['success']:
                return json.dumps(result, indent=2, ensure_ascii=False, default=str)
            else:
                return result['message']
                
        except Exception as e:
            return f"è®­ç»ƒå¤±è´¥: {e}"
    
    def get_summary_interface():
        """è·å–æ‘˜è¦ç•Œé¢"""
        summary = agent.get_data_summary()
        return json.dumps(summary, indent=2, ensure_ascii=False, default=str)
    
    # åˆ›å»ºGradioç•Œé¢
    with gr.Blocks(title="æ•°æ®åˆ†æAgent", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ğŸ“Š æ•°æ®åˆ†æAgent")
        gr.Markdown("å…¨é¢çš„æ•°æ®ç§‘å­¦è§£å†³æ–¹æ¡ˆï¼šæ•°æ®å¤„ç†ã€åˆ†æã€å¯è§†åŒ–å’Œæœºå™¨å­¦ä¹ ")
        
        with gr.Tabs():
            # æ•°æ®åŠ è½½æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ“ æ•°æ®åŠ è½½"):
                with gr.Row():
                    with gr.Column():
                        data_file = gr.File(label="é€‰æ‹©æ•°æ®æ–‡ä»¶", file_types=[".csv", ".json", ".xlsx"])
                        data_type = gr.Dropdown(
                            label="æ•°æ®ç±»å‹",
                            choices=["auto", "csv", "json", "excel"],
                            value="auto"
                        )
                        dataset_name_input = gr.Textbox(
                            label="æ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼‰",
                            placeholder="ç•™ç©ºåˆ™è‡ªåŠ¨ç”Ÿæˆ"
                        )
                        load_btn = gr.Button("ğŸ“‚ åŠ è½½æ•°æ®", variant="primary")
                    
                    with gr.Column():
                        load_result = gr.Textbox(
                            label="åŠ è½½ç»“æœ",
                            lines=3,
                            interactive=False
                        )
                
                with gr.Row():
                    with gr.Column():
                        data_info = gr.Textbox(
                            label="æ•°æ®é›†ä¿¡æ¯",
                            lines=15,
                            interactive=False
                        )
                    
                    with gr.Column():
                        data_sample = gr.Textbox(
                            label="æ•°æ®æ ·æœ¬",
                            lines=15,
                            interactive=False
                        )
                
                load_btn.click(
                    load_data_interface,
                    inputs=[data_file, data_type, dataset_name_input],
                    outputs=[load_result, data_info, data_sample]
                )
            
            # æ•°æ®åˆ†ææ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ” æ•°æ®åˆ†æ"):
                with gr.Row():
                    with gr.Column():
                        analyze_dataset = gr.Dropdown(
                            label="é€‰æ‹©æ•°æ®é›†",
                            choices=[],
                            interactive=True
                        )
                        analyze_type = gr.Dropdown(
                            label="åˆ†æç±»å‹",
                            choices=["descriptive", "correlation", "distribution", 
                                   "missing_values", "outliers"],
                            value="descriptive"
                        )
                        analyze_btn = gr.Button("ğŸ” å¼€å§‹åˆ†æ", variant="primary")
                    
                    with gr.Column():
                        analyze_result = gr.Textbox(
                            label="åˆ†æç»“æœ",
                            lines=25,
                            interactive=False
                        )
                
                analyze_btn.click(
                    analyze_data_interface,
                    inputs=[analyze_dataset, analyze_type],
                    outputs=[analyze_result]
                )
            
            # æ•°æ®å¯è§†åŒ–æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ“ˆ æ•°æ®å¯è§†åŒ–"):
                with gr.Row():
                    with gr.Column():
                        viz_dataset = gr.Dropdown(
                            label="é€‰æ‹©æ•°æ®é›†",
                            choices=[],
                            interactive=True
                        )
                        viz_type = gr.Dropdown(
                            label="å¯è§†åŒ–ç±»å‹",
                            choices=["histogram", "scatter", "box", "correlation_heatmap", 
                                   "line", "bar"],
                            value="histogram"
                        )
                        viz_column = gr.Textbox(
                            label="åˆ—åï¼ˆç”¨äºç›´æ–¹å›¾ã€ç®±çº¿å›¾ï¼‰",
                            placeholder="è¾“å…¥åˆ—å"
                        )
                        viz_x_column = gr.Textbox(
                            label="Xè½´åˆ—åï¼ˆç”¨äºæ•£ç‚¹å›¾ã€çº¿å›¾ç­‰ï¼‰",
                            placeholder="è¾“å…¥Xè½´åˆ—å"
                        )
                        viz_y_column = gr.Textbox(
                            label="Yè½´åˆ—åï¼ˆç”¨äºæ•£ç‚¹å›¾ã€çº¿å›¾ç­‰ï¼‰",
                            placeholder="è¾“å…¥Yè½´åˆ—å"
                        )
                        viz_btn = gr.Button("ğŸ“Š åˆ›å»ºå¯è§†åŒ–", variant="primary")
                    
                    with gr.Column():
                        viz_result = gr.Textbox(
                            label="å¯è§†åŒ–ç»“æœ",
                            lines=3,
                            interactive=False
                        )
                        viz_display = gr.HTML(label="å¯è§†åŒ–å›¾è¡¨")
                
                viz_btn.click(
                    visualize_data_interface,
                    inputs=[viz_dataset, viz_type, viz_column, viz_x_column, viz_y_column],
                    outputs=[viz_result, viz_display]
                )
            
            # æ•°æ®æ¸…æ´—æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ§¹ æ•°æ®æ¸…æ´—"):
                with gr.Row():
                    with gr.Column():
                        clean_dataset = gr.Dropdown(
                            label="é€‰æ‹©æ•°æ®é›†",
                            choices=[],
                            interactive=True
                        )
                        clean_operations = gr.Textbox(
                            label="æ¸…æ´—æ“ä½œï¼ˆJSONæ ¼å¼ï¼‰",
                            placeholder='[{"type": "fill_nulls", "params": {"method": "mean"}}]',
                            lines=10
                        )
                        clean_btn = gr.Button("ğŸ§¹ å¼€å§‹æ¸…æ´—", variant="primary")
                    
                    with gr.Column():
                        clean_result = gr.Textbox(
                            label="æ¸…æ´—ç»“æœ",
                            lines=3,
                            interactive=False
                        )
                        clean_info = gr.Textbox(
                            label="æ¸…æ´—åæ•°æ®ä¿¡æ¯",
                            lines=15,
                            interactive=False
                        )
                
                clean_btn.click(
                    clean_data_interface,
                    inputs=[clean_dataset, clean_operations],
                    outputs=[clean_result, clean_info]
                )
            
            # æœºå™¨å­¦ä¹ æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ¤– æœºå™¨å­¦ä¹ "):
                with gr.Row():
                    with gr.Column():
                        ml_model_name = gr.Textbox(
                            label="æ¨¡å‹åç§°",
                            placeholder="è¾“å…¥æ¨¡å‹åç§°"
                        )
                        ml_algorithm = gr.Dropdown(
                            label="ç®—æ³•",
                            choices=["logistic_regression", "random_forest", "svm", 
                                   "linear_regression", "ridge", "kmeans"],
                            value="random_forest"
                        )
                        ml_task_type = gr.Dropdown(
                            label="ä»»åŠ¡ç±»å‹",
                            choices=["classification", "regression", "clustering"],
                            value="classification"
                        )
                        ml_dataset = gr.Dropdown(
                            label="é€‰æ‹©æ•°æ®é›†",
                            choices=[],
                            interactive=True
                        )
                        ml_target = gr.Textbox(
                            label="ç›®æ ‡åˆ—ï¼ˆç›‘ç£å­¦ä¹ ï¼‰",
                            placeholder="è¾“å…¥ç›®æ ‡åˆ—å"
                        )
                        ml_params = gr.Textbox(
                            label="ç®—æ³•å‚æ•°ï¼ˆJSONæ ¼å¼ï¼‰",
                            placeholder='{"n_estimators": 100}',
                            lines=5
                        )
                        ml_train_btn = gr.Button("ğŸš€ è®­ç»ƒæ¨¡å‹", variant="primary")
                    
                    with gr.Column():
                        ml_result = gr.Textbox(
                            label="è®­ç»ƒç»“æœ",
                            lines=25,
                            interactive=False
                        )
                
                ml_train_btn.click(
                    train_model_interface,
                    inputs=[ml_model_name, ml_algorithm, ml_task_type, 
                           ml_dataset, ml_target, ml_params],
                    outputs=[ml_result]
                )
            
            # é¡¹ç›®æ‘˜è¦æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ“‹ é¡¹ç›®æ‘˜è¦"):
                with gr.Row():
                    with gr.Column():
                        refresh_summary_btn = gr.Button("ğŸ”„ åˆ·æ–°æ‘˜è¦", variant="primary")
                    
                    with gr.Column():
                        summary_display = gr.Textbox(
                            label="é¡¹ç›®æ‘˜è¦",
                            lines=25,
                            interactive=False
                        )
                
                refresh_summary_btn.click(
                    get_summary_interface,
                    outputs=[summary_display]
                )
        
        # å®šæœŸæ›´æ–°æ•°æ®é›†é€‰æ‹©æ¡†
        def update_dataset_choices():
            choices = agent.session_state['loaded_datasets']
            return (
                gr.Dropdown(choices=choices),
                gr.Dropdown(choices=choices),
                gr.Dropdown(choices=choices),
                gr.Dropdown(choices=choices)
            )
        
        # å¯ä»¥æ·»åŠ å®šæ—¶æ›´æ–°é€»è¾‘
        interface.load(
            update_dataset_choices,
            outputs=[analyze_dataset, viz_dataset, clean_dataset, ml_dataset]
        )
    
    return interface


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ•°æ®åˆ†æAgent")
    parser.add_argument("--mode", choices=['web', 'cli'], default='web',
                       help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--host", default="127.0.0.1", help="æœåŠ¡å™¨ä¸»æœº")
    parser.add_argument("--port", type=int, default=7866, help="æœåŠ¡å™¨ç«¯å£")
    parser.add_argument("--share", action="store_true", help="åˆ›å»ºå…¬å…±é“¾æ¥")
    
    args = parser.parse_args()
    
    if args.mode == 'cli':
        # CLIæ¨¡å¼ï¼ˆç®€åŒ–å®ç°ï¼‰
        agent = DataAnalysisAgent()
        print("æ•°æ®åˆ†æAgent CLIæ¨¡å¼")
        print("å¯ç”¨å‘½ä»¤: load, analyze, visualize, train, summary")
        
        while True:
            try:
                command = input(">>> ").strip().lower()
                if command == 'quit' or command == 'exit':
                    break
                elif command == 'summary':
                    summary = agent.get_data_summary()
                    print(json.dumps(summary, indent=2, ensure_ascii=False, default=str))
                else:
                    print("å‘½ä»¤ä¸æ”¯æŒï¼Œè¯·ä½¿ç”¨ web æ¨¡å¼è·å¾—å®Œæ•´åŠŸèƒ½")
            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"é”™è¯¯: {e}")
    
    else:
        # Webç•Œé¢æ¨¡å¼
        agent = DataAnalysisAgent()
        
        # åˆ›å»ºWebç•Œé¢
        interface = create_gradio_interface(agent)
        
        # å¯åŠ¨æœåŠ¡å™¨
        logger.info(f"å¯åŠ¨æ•°æ®åˆ†æAgent Webç•Œé¢: http://{args.host}:{args.port}")
        interface.launch(
            server_name=args.host,
            server_port=args.port,
            share=args.share,
            show_error=True
        )


if __name__ == "__main__":
    main()